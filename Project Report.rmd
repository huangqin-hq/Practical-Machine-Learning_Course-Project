# Practical-Machine-Learning_Course-Project

## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Prediction Feature Extraction
In this project, the outcome of the prediction model is the classification of the way that the barbell lift is performed based on the sensor measurements. The model outcome is represented in the 'class' variable in the training set which has 5 levels including "A","B","C","D", and "E". For model feature extraction, only raw sensor measurements (accelerometer, gyroscope, and magnetometer readings) from belt, forearm, arm, and dumbell at Euler angles (roll, pitch, and yaw) are used, omitting summary statistics variables. Below keywards are used for feature extraction: "^roll", "^pitch", "^yaw", "^accel", "^gyros", "^magnet" and "^total". Using this approach, 52 predictors varaibles can be extracted from training set.

```{r}
predictorIdx <- c(grep("^roll", names(training)), grep("^pitch", names(training)), grep("^yaw", names(training)), grep("^accel", names(training)), grep("^gyros", names(training)), grep("^magnet", names(training)), grep("^total", names(training)))
```

## Training Dataset and Cross-Validation
10-fold cross-validation algorithm is used imporove the model performance. The training data set is partitioned into a cv training set(80%) and cv test set(20%).

```{r}
set.seed(123)
inTrain <- createDataPartition(y = trainPredSet$classe, p = 0.8, list = FALSE)
cvTrain <- trainPredSet[inTrain, ]
cvTest <- trainPredSet[-inTrain, ]
fitCtrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
```

## Prediction Model Selection
In view of large number of predictors and large training data set, Quadratic Discriminant Analysis is used to generate reasonalbly good accuracy within reasonalble processing time.

```{r}
modFit <- train(classe ~ ., data = cvTrain, method = "qda", trControl = fitCtrl)
```
Quadratic Discriminant Analysis:
15699 samples
52 predictor
5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 14130, 14129, 14130, 14130, 14129, 14128, ... 
Resampling results:
Accuracy   Kappa    
0.8932546  0.8652128

## Expected Out-of-Sample Error
By applying the cross-validationg training set into the model, accuracy is 90%. Thus the expected out-of-sample error is more than 10% as typically the prediction model achieves less accuracy on new data sets.
```{r}
predTrain <- predict(modFit, newdata = cvTrain)
equalPredTrain <- (predTrain == cvTrain$classe)
print(sum(equalPredTrain)/length(equalPredTrain))
```
0.9001847

```{r}
confusionMatrix(data = predTrain, reference = cvTrain$classe)
```
Confusion Matrix and Statistics:

          Reference
Prediction    A    B    C    D    E
         A 4137  139    0    4    0
         B  169 2568  119   10   86
         C   83  303 2603  331  107
         D   67    9    9 2200   69
         E    8   19    7   28 2624

Overall Statistics:                         
               Accuracy : 0.9002          
                 95% CI : (0.8954, 0.9048)
    No Information Rate : 0.2843          
    P-Value [Acc > NIR] : < 2.2e-16                                 
                  Kappa : 0.874           
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:
                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9267   0.8453   0.9507   0.8550   0.9092
Specificity            0.9873   0.9697   0.9364   0.9883   0.9952
Pos Pred Value         0.9666   0.8699   0.7596   0.9346   0.9769
Neg Pred Value         0.9714   0.9631   0.9890   0.9720   0.9799
Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2635   0.1636   0.1658   0.1401   0.1671
Detection Prevalence   0.2726   0.1880   0.2183   0.1499   0.1711
Balanced Accuracy      0.9570   0.9075   0.9436   0.9217   0.9522

## Prediction Results on Test Dataset
Apply the predition model to testing data set to generate predition results:

```{r}
testPrediction <- predict(modFit, newdata = testing)
table(testing$problem_id, testPrediction)
```
testPrediction 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
             A 1 1 0 1 1 0 0 0 1  1  0  0  0  1  0  0  1  0  0  0
             B 0 0 1 0 0 0 0 1 0  0  1  0  1  0  0  0  0  1  1  1
             C 0 0 0 0 0 0 0 0 0  0  0  1  0  0  0  0  0  0  0  0
             D 0 0 0 0 0 0 1 0 0  0  0  0  0  0  0  0  0  0  0  0
             E 0 0 0 0 0 1 0 0 0  0  0  0  0  0  1  1  0  0  0  0

testPrediction (problem_id 1 to 20):
A A B A A E D B A A B C B A E E A B B B

## Summary
